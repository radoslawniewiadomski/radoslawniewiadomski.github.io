<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html><head>

 
  <title>Datasets</title>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <link rel="stylesheet" type="text/css" href="style.css" media="screen">
  <meta name="robots" content="follow,index">  
 </head><body>


<div id="container">
<div id="navcontainer">
<ul id="navlist">
<li><a href="index.html">Home</a></li>
<li><a href="research.html">Research</a></li>
<li><a href="papers.html">Publications</a></li>
<li><a href="others.html">Datasets</a></li>
<li><a href="#"></a></li>
</ul>
</div>

<b><a name="datasets"></a><H1>Datasets</H1></b>
<p id="content27">
I participated in the creation of several mutlimodal datasets that are available for the reseach purposes:
</p>

<ul>
<li>
<b><a href="https://github.com/estiei/PIFE-Perceived-Intensity-of-Facial-Expression-Dataset">The Perceived Intensity of Facial Expressions Dataset (PIFED)</a></b> contains the facial data of short extracts from movies rated in terms of intensity and variability by 5 raters.  The total number of segments is 400. The Action Units and raters' scores can be downloaded <a href="https://github.com/estiei/PIFE-Perceived-Intensity-of-Facial-Expression-Dataset">here</a>.
<BR>
More details in this <a href="https://radoslawniewiadomski.github.io/papers/ACII24_tiulenevaetal.pdf">paper</a>.
<BR>
<BR>
</li>

<li>
<b><a href="https://gitlab.com/flarradet/epsdi">Emotional Physiological Signal Database built In-the-wild (EPSDI)</a></b> contains several physiological signals collected with off-the-shelf wearable sensors and emotional self-reports for 15 subjects over a period of up to 7 days each.
It can be downoladed <a href="https://gitlab.com/flarradet/epsdi">here</a>.
<BR>
More details in this <a href="https://www.frontiersin.org/articles/10.3389/fcomp.2023.1285690">paper</a>.
<BR>
<BR>
</li>

<li>
<b><a href="https://zenodo.org/records/6768641
">The Digital Commensality Data-set</a></b> consists of facial activity data of 11 pairs of persons sharing a meal online through a videoconferencing software and self-reported qualitative and qualitative measures of their commensal experience (Computer-Mediated Communication questionnaire and Digital Commensality questionnaire). Facial activity data is extracted using the OpenFace tool.
It can be downoladed <a href="https://zenodo.org/records/6768641">here</a>.
<BR>
More details in this <a href="https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2022.911000/full">paper</a>.
<BR>
<BR>
</li>
<li>
<b><a href="http://www.infomus.org/ILHAIRE/mmli/">Multimodal Multiperson Corpus of Laughter in Interaction (MMLI)</a></b> contains full-body data of hilarious laughter. It contains 500 segments of full-body motion capture of subjects who participated in several activities, e.g., playing social games such as "barbichette" or pictionary game, etc. 
The total duration of the segments is more than 70 minutes. The motion data can be downloaded <a href="http://www.infomus.org/ILHAIRE/mmli/">here</a>.
<BR>
More details in this <a href="https://radoslawniewiadomski.github.io/papers/THMS16_niewiadomskietal_draft.pdf">paper</a>.
<BR>
<BR>
</li>
<li>
<b><a href="http://dance.dibris.unige.it/index.php/dance-datasets/dance-dataset-1">Multimodal Dataset of Lightness and Fragility</a></b> is composed of short segments containing full-body movements displaying two expressive qualities.
In total we collected 150 segments by 13 participants. The data consists of multiple 3D accelerometer data, video channels, respiration audio and EMG signals.
The data can be downloaded <a href="http://dance.dibris.unige.it/index.php/dance-datasets/dance-dataset-1">here</a>.
<BR>
More details in this <a href="https://radoslawniewiadomski.github.io/papers/ICMI17_niewiadomskietal.pdf">paper</a>.
<BR>
<BR>
</li>
<li>
<b><a href="http://www.infomus.org/karate/eyesweb_dataset_karate_eng.php">UNIGE  Katas (Karate) Dataset</a></b> contains full-body movements of athletes performing katas. IT is composed of 32 minutes of 3D MoCap data at 250 Hz performed by several athletes with the different levels of experience. It can be downloaded <a href="http://www.infomus.org/karate/eyesweb_dataset_karate_eng.php">here</a>.
<BR>
More details in this <a href="https://dl.acm.org/doi/pdf/10.1145/3132369">paper</a>.
<BR>
<BR>
</li>
</ul>

<b><a name="questionaires"></a><H1>Questionaires</H1></b>
<ul>
<li>
<b></b> 
<a href="https://dl.acm.org/doi/suppl/10.1145/3686215.3686220/suppl_file/supplementary.zip">here</a>.
<BR>
More details in this <a href="https://dl.acm.org/doi/pdf/10.1145/3686215.3686220">paper</a>.
<BR>
</li>
</ul>
<div id="content27">
Check also my profiles on: 
<a href="https://osf.io/bnfrd/">OSF</a>, and
<a href="https://zenodo.org/search?q=metadata.creators.person_or_org.name%3A%22Radoslaw%20Niewiadomski%22&l=list&p=1&s=10&sort=bestmatch
">Zenodo</a>.

</div></div>
</body>
</html>